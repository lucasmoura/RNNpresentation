\documentclass[10pt]{beamer}
\usetheme{metropolis}
% all imports
\input{all_imports}

\AtBeginEnvironment{quote}{\singlespacing}

% new commands
\input{all_new_commands}

% definitions
\input{definitions/colors}
\input{definitions/styles}

\input{header}

\begin{document}

\maketitle

\section{Introduction}

\begin{frame}[fragile]{Motivation}
\begin{itemize}
\item \alert{Deep Learning} is a growing field with state-of-the-art results in
    several areas.
\vspace{0.5cm}
\item Image Classification, Machine Translation
\end{itemize}
\end{frame}

\section{However...}

\begin{frame}[fragile]
\begin{itemize}
\item Training \alert{Deep Learning} models require a huge amount of labeled data
\vspace{0.5cm}
\item For the task of image classification on the ImageNet database, 1.2 million
    labeled images were used \cite{imagenet}
\vspace{0.5cm}
\item This restriction causes huge difficulties on applying Deep Learning
    techniques to a wide range of problems, such as \alert{Sentiment Analysis}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Sentiment Analysis}
\begin{itemize}
\item Verify if a text is expressing negative or positive feelings.
\vspace{0.5cm}
\item Huge amount of data, but few labeled.
\end{itemize}
\end{frame}

\section{Active Learning}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_ml_model.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_unlabeled.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_oracle.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_labeled.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning - CEAL}
    \input{TikzFiles/ceal_cycle.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Uncertainty measurement}
\begin{itemize}
\item To select informative samples, it is necessary to measure the
    \alert{uncertainty} of the model prediction.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Neural Network}
    \input{TikzFiles/neural_network}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network - prediction}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/bayesian_neural_network.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network - prediction}
    \input{TikzFiles/bayesian_classification.tex}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network - prediction}
\begin{itemize}
\item Training Bayesian networks is a costly process
\vspace{0.5cm}
\item Use techniques such as Variational Inference
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network}
\begin{itemize}
\item What if we could extract uncertainty measurements from current Deep
    Learning models if they use stochastic regularization techniques such as
    \alert{Dropout} ?
\vspace{0.5cm}
\item Uncertainty in Deep Learning (Yarin Gal, 2017)
\end{itemize}
\end{frame}

\section{Dropout}

\begin{frame}[fragile]{Dropout}
\begin{itemize}
\item During training some weights are dropped from the network
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dropout}
    \input{TikzFiles/dropout_1.tex}
\end{frame}

\begin{frame}[fragile]{Dropout}
    \input{TikzFiles/dropout_2.tex}
\end{frame}

\begin{frame}[fragile]{Dropout}
\begin{itemize}
    \item The optimization function of Neural Networks using \alert{Dropout} is practically the same
        as the optimization function of a Network trained with Variational
        Inference.
    \vspace{0.5cm}
    \item Therefore it is possible to extract uncertainty measures from these
        networks, a technique called \alert{Monte Carlo Dropout}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Monte Carlo Dropout - Prediction}
    \input{TikzFiles/dropout_classification.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty_dropout.png}
    \end{figure}
\end{frame}

\section{Experimental Design}

\begin{frame}[fragile]{Research Contributions}
\begin{itemize}
\item Evaluation of the use of Active Learning together wiht Deep Learning for the task
      of sentiment analysis.
\item Study of how Monte Carlo Dropout can provide uncertainty measurements for the LSTM Deep
      Learning architecture.
\item Practical considerations when applying Deep Learning with Active Learning
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Research Questions}
\begin{itemize}
    \item \alert{Q1}: On the task of sentiment analysis, does modelling the uncertainty measurement of the
    model using the Monte Carlo Dropout technique help us achieve a better accuracy value in
    the Active Learning context ?
    \vspace{0.5cm}
    \item \alert{Q2}: Does Monte Carlo Dropout
    provides best uncertainty measurements then using the soft-
    max output as a uncertainty measurement, the classical approach used in DL models ?
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Datasets}
\begin{itemize}
    \item Large Movie Review Dataset
    \begin{itemize}
    \item 25000 train reviews and 25000 test reviews
    \item Both train and test datasets have an equal number of positive and
        negative reviews
    \end{itemize}
    \vspace{0.5cm}
    \item Subjectivity Dataset
    \begin{itemize}
    \item 10000 movie reviews divided into subjective and objective text.
    \item Dataset perfectly balanced
    \item Reviews hava an average size of 20 words
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Network Archictecture}
    \input{TikzFiles/network_architecture}
\end{frame}

\begin{frame}[fragile]{Experimental Design}
    \input{TikzFiles/experimental_design.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.6]{images/active_learning_comp_graph.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning Experiments Parameters}
\begin{table}[]
\centering
\begin{tabular}{lll}
\toprule
Dataset & Labeled Group & Unlabeled Group\\
\midrule
\rowcolor{black!20}  Large Movie Review Dataset & 225 & 22275\\
Subjectivity Dataset & 10 & 8090\\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]{Active Learning Experiments Parameters}
\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ll}
\toprule
Parameter & Description\\
\midrule
\rowcolor{black!20} Unlabeled Data Queries (Q) & The number of example we will select from the unlabeled group\\
\rowcolor{black!20} & to be labeled by the oracle.\\
Number of epochs (EPO) & At each AL cycle, we will train our model for a given number of\\
& epochs. This variable defines this quantity.\\
\rowcolor{black!20} Dropout Values (DROP) & The dropout probability for the weights in our network.\\
Number of Active Learning Cycles (NC) & The number of AL cycles we have run for a given experiment.\\
\bottomrule
\end{tabular}%
}
\end{table}
\end{frame}


\section{Experiments Evaluation}

\begin{frame}[fragile]{First Iteration}
    \begin{itemize}
        \item Use Large Movie Review Dataset
        \item Train only a Random and ALS\_I model (Mutual Information)
        \item Use Hyperparameters found with our baseline model
        \item Parameters
            \begin{itemize}
            \item \textbf{Q} = 50
            \item \textbf{EPO} = 16
            \item \textbf{NC} = 50
            \end{itemize}
        \item Random and ALS\_I curve are almost identical
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Second Iteration}
\begin{itemize}
    \item Increase number of epochs in experiments
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Second Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n200_comparison}
\end{figure}

\hspace{1.5cm} $\bullet$ \textbf{Q} = 50 \hspace{1cm} $\bullet$ \textbf{EPO} = 200 \hspace{1cm} $\bullet$ \textbf{NC} = 50
\end{frame}

\begin{frame}[fragile]{Third Iteration}
\begin{itemize}
    \item Decrease number of unlabeled examples added to training dataset
    \item Decrease number of epochs
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Third Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n150_q10_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm}
$\bullet$ \textbf{NC} = 50 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Third Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n150_q10_std_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm}
$\bullet$ \textbf{NC} = 50 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Fourth Iteration}
\begin{itemize}
    \item Compare all metrics
    \item Use CEAL approach
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Fourth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n150_q10_full_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm} $\bullet$ \textbf{NC} = 100 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Fourth Iteration}
    \input{TikzFiles/ceal_cycle.tex}
\end{frame}

\begin{frame}[fragile]{Fourth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n150_q10_ceal_comparison}
\end{figure}

\end{frame}

\begin{frame}[fragile]{Fifth Iteration}
\begin{itemize}
    \item Compare softmax metric
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Fifth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/acl_n150_q10_softmax_bald_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm} $\bullet$ \textbf{NC} = 100 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Sixth Iteration}
\begin{itemize}
    \item Use Subjectivity Dataset
    \item Compare all metrics
    \item Use CEAL approach
    \item Make softmax comparison
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Sixth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/subj_n150_q10_full_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm} $\bullet$ \textbf{NC} = 400 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Sixth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/subj_n150_q10_ceal_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm} $\bullet$ \textbf{NC} = 400 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\begin{frame}[fragile]{Sixth Iteration}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/subj_n150_q10_softmax_bald_comparison}
\end{figure}

\hspace{0.5cm} $\bullet$ \textbf{Q} = 10 \hspace{0.5cm} $\bullet$ \textbf{EPO} = 150 \hspace{0.5cm} $\bullet$ \textbf{NC} = 400 \hspace{0.5cm} \textbf{DROP} = 0.5
\end{frame}

\section{Conclusion}

\begin{frame}[fragile]{Conclusion}
\begin{itemize}
\item Measuring the uncertainty of sample using the Monte Carlo Dropout has created better
      accuracy curves than using both random and softmax.
\item We have positive results for both of our research questions
\item Not consistent result for both datasets
\end{itemize}
\end{frame}

\section{However}

\begin{frame}[fragile]{Active Learning Problems}
\begin{itemize}
\item LSTM is a bad archictecture for Active Learning
\item Retraining Deep Learning algorithms each cycle does not scale
\item Active Learning is biased approach
\item Feel engineering approaches for Active Learning
\item Costly framework and though to apply it to practical problems
\item Huge number of variables to monitor
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Future Work}
\begin{itemize}
\item Explore CEAL framework with Monte Carlo Dropout for other areas, such as image recognition
\item Evaluate more Active Learning parameters
\item Monitor the types of sentences the model is choosing from the unlabeled group
\item \alert{Engineering work}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{References}
  \bibliography{demo}
  \bibliographystyle{abbrv}
\end{frame}

\end{document}
