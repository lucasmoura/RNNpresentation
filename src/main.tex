\documentclass[10pt]{beamer}
\usetheme{metropolis}
% all imports
\input{all_imports}

\AtBeginEnvironment{quote}{\singlespacing}

% new commands
\input{all_new_commands}

% definitions
\input{definitions/colors}
\input{definitions/styles}

\input{header}

\begin{document}

\maketitle

\section{Introduction}

\begin{frame}[fragile]{Motivation}
\begin{itemize}
\item \alert{Deep Learning} is a growing field with state-of-the-art results in
    several areas.
\vspace{0.5cm}
\item Image Classification, Machine Translation
\end{itemize}
\end{frame}

\section{However...}

\begin{frame}[fragile]
\begin{itemize}
\item Training \alert{Deep Learning} models require a huge amount of labeled data
\vspace{0.5cm}
\item For the task of image classification on the ImageNet database, 1.2 million
    labeled images were used \cite{imagenet}
\vspace{0.5cm}
\item This restriction causes huge difficulties on applying Deep Learning
    techniques to a wide range of problems, such as \alert{Sentiment Analysis}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Sentiment Analysis}
\begin{itemize}
\item Verify if a text is expressing negative or positive feelings.
\vspace{0.5cm}
\item Huge amount of data, but few labeled.
\end{itemize}
\end{frame}

\section{Active Learning}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_ml_model.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_unlabeled.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_oracle.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_labeled.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Uncertainty measurement}
\begin{itemize}
\item To select informative samples, it is necessary to measure the
    \alert{uncertainty} of the model prediction.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Neural Network}
    \input{TikzFiles/neural_network}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/bayesian_neural_network.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network}
    \input{TikzFiles/bayesian_classification.tex}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network}
\begin{itemize}
\item Training Bayesian networks is a costly process
\vspace{0.5cm}
\item Use techniques such as Variational Inference and Monte Carlo Estimation
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Bayesian Neural Network}
\begin{itemize}
\item What if we could extract uncertainty measurements from current Deep
    Learning models if they use stochastic regularization techniques such as
    \alert{Dropout} ?
\vspace{0.5cm}
\item Uncertainty in Deep Learning (Yarin Gal, 2017)
\end{itemize}
\end{frame}

\section{Dropout}

\begin{frame}[fragile]{Monte Carlo Dropout}
    \input{TikzFiles/dropout_classification.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.3]{images/active_learning_uncertainty_dropout.png}
    \end{figure}
\end{frame}

\section{Experimental Design}

\begin{frame}[fragile]{Objective}
\begin{itemize}
    \item Combine Monte Carlo Dropout with Active Learning for the task of
        Sentiment Analysis and answer the folling research questions:
        \vspace{0.5cm}

        \begin{itemize}
        \item \alert{Q1}: On the task of sentiment analysis, can we achieve the same
            accuracy of a standard Deep Learning model by using Active Learning
            with uncertainty measurements, but with fewer labeled data ?
        \item \alert{Q2}: Does modelling uncertainty in a Deep Learning model helps
            achieving a better result when using Active Learning ?
        \end{itemize}
    \vspace{0.5cm}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dataset}
\begin{itemize}
    \item Large Movie Review Dataset
    \vspace{0.5cm}
    \item 25000 train reviews and 25000 test reviews
    \vspace{0.5cm}
    \item Both train and test datasets have an equal number of positive and
        negative reviews
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dataset}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.6]{images/train_positive_graph.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Experimental Design}
    \input{TikzFiles/experimental_design.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{itemize}
        \item \alert{Q1}: On the task of sentiment analysis, can we achieve the same
            accuracy of a standard Deep Learning model by using Active Learning
            with uncertainty measurements, but with fewer labeled data ?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.6]{images/active_learning_comp_graph.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{itemize}
        \item \alert{Q2}: Does modelling uncertainty in a Deep Learning model helps
            achieving a better result when using Active Learning ?
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.6]{images/active_learning_selection_comp_graph.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.22]{images/roadmap.png}
    \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{References}
  \bibliography{demo}
  \bibliographystyle{abbrv}
\end{frame}

\section{Backup Slides}

\begin{frame}[fragile]{Architecture}
    \input{TikzFiles/architecture.tex}
\end{frame}

\begin{frame}[fragile]{Active Learning}
    \begin{figure}[htp]
        \centering
        \includegraphics[scale=0.6]{images/parcial_results.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]{Dropout}
\begin{itemize}
\item During training some weights are dropped from the network
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Dropout}
    \input{TikzFiles/dropout_1.tex}
\end{frame}

\begin{frame}[fragile]{Dropout}
    \input{TikzFiles/dropout_2.tex}
\end{frame}

\begin{frame}[fragile]{Dropout}
\begin{itemize}
    \item The optimization function of Neural Networks using \alert{Dropout} is practically the same
        as the optimization function of a Network trained with Variational
        Inference.
    \vspace{0.5cm}
    \item Therefore it is possible to extract uncertainty measures from these
        networks, a technique called \alert{Monte Carlo Dropout}.
\end{itemize}
\end{frame}

\end{document}
